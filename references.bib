@article{Burke_hybrid,
author = {Burke, Robin},
year = {2002},
month = {11},
pages = {},
title = {Hybrid Recommender Systems: Survey and Experiments},
volume = {12},
journal = {User Modeling and User-Adapted Interaction},
doi = {10.1023/A:1021240730564}
}
@inproceedings{coldstart,
author = {Kluver, Daniel and Konstan, Joseph A.},
title = {Evaluating Recommender Behavior for New Users},
year = {2014},
isbn = {9781450326681},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2645710.2645742},
doi = {10.1145/2645710.2645742},
abstract = {The new user experience is one of the important problems in recommender systems. Past
work on recommending for new users has focused on the process of gathering information
from the user. Our work focuses on how different algorithms behave for new users.
We describe a methodology that we use to compare representatives of three common families
of algorithms along eleven different metrics. We find that for the first few ratings
a baseline algorithm performs better than three common collaborative filtering algorithms.
Once we have a few ratings, we find that Funk's SVD algorithm has the best overall
performance. We also find that ItemItem, a very commonly deployed algorithm, performs
very poorly for new users. Our results can inform the design of interfaces and algorithms
for new users.},
booktitle = {Proceedings of the 8th ACM Conference on Recommender Systems},
pages = {121–128},
numpages = {8},
keywords = {user cold start, new user experience, new user problem, profile size, recommender systems, evaluation},
location = {Foster City, Silicon Valley, California, USA},
series = {RecSys '14}
}

@inproceedings{annoy,
author = {Indyk, Piotr and Motwani, Rajeev},
title = {Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality},
year = {1998},
isbn = {0897919629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/276698.276876},
doi = {10.1145/276698.276876},
booktitle = {Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing},
pages = {604–613},
numpages = {10},
location = {Dallas, Texas, USA},
series = {STOC '98}
}

@article{onecycle,
  author    = {Leslie N. Smith and
               Nicholay Topin},
  title     = {Super-Convergence: Very Fast Training of Residual Networks Using Large
               Learning Rates},
  journal   = {CoRR},
  volume    = {abs/1708.07120},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.07120},
  archivePrefix = {arXiv},
  eprint    = {1708.07120},
  timestamp = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-07120.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{ffnn,
  abstract = {Although interest in machine learning has reached a high point, lofty expectations often scuttle projects before they get very far. How can machine learning---especially deep neural networks---make a real difference in your organization? This hands-on guide not only provides the most practical information available on the subject, but also helps you get started building efficient deep learning networks. The authors provide theory on deep learning before introducing their open-source Deeplearning4j (DL4J) library for developing production-class workflows. Through real-world examples, you will learn methods and strategies for training deep network architectures and running deep learning workflows on Spark and Hadoop with DL4J.},
  added-at = {2017-11-10T09:58:27.000+0100},
  address = {Beijing},
  author = {Patterson, Josh and Gibson, Adam},
  biburl = {https://www.bibsonomy.org/bibtex/2861e7eedb0f4c75409500abe08c9ad2d/flint63},
  file = {eBook:2017/PattersonGibson17.pdf:PDF;O'Reilly Product page:http\://shop.oreilly.com/product/0636920035343/:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/1491914254/:URL;Related Web site:https\://deeplearning4j.org/:URL},
  groups = {public},
  interhash = {aaa0845182fff81206ef3b2dbc0beaea},
  intrahash = {861e7eedb0f4c75409500abe08c9ad2d},
  isbn = {978-1-4919-1425-0},
  keywords = {01841 102 safari book numerical ai software development learn java tool},
  publisher = {O'Reilly},
  timestamp = {2018-04-16T11:39:52.000+0200},
  title = {Deep Learning: A Practitioner's Approach},
  url = {https://www.safaribooksonline.com/library/view/deep-learning/9781491924570/},
  username = {flint63},
  year = 2017
}

@online{EUdataregulations2018,
    title = {2018 reform of EU data protection rules},
    url = {https://ec.europa.eu/commission/sites/beta-political/files/data-protection-factsheet-changes_en.pdf},
    organization = {European Commission},
    date = {2018-05-25},
    urldate = {2019-06-17}
}
    
@online{netflix,
  author = {Netflix},
  title = {Netflix prize},
  year = 2007,
  url = {https://www.netflixprize.com/},
  urldate = {2021-07-19}
}
@inproceedings{alsnetflix,
author = {Zhou, Yunhong and Wilkinson, Dennis and Schreiber, Robert and Pan, Rong},
year = {2008},
month = {06},
pages = {337-348},
title = {Large-Scale Parallel Collaborative Filtering for the Netflix Prize},
isbn = {978-3-540-68865-5},
doi = {10.1007/978-3-540-68880-8_32}
}
@InProceedings{svd,
author="Cao, Jian
and Hu, Hengkui
and Luo, Tianyan
and Wang, Jia
and Huang, May
and Wang, Karl
and Wu, Zhonghai
and Zhang, Xing",
editor="Zhang, Xing
and Wu, Zhonghai
and Sha, Xingmian",
title="Distributed Design and Implementation of SVD++ Algorithm for E-commerce Personalized Recommender System",
booktitle="Embedded System Technology",
year="2015",
publisher="Springer Singapore",
address="Singapore",
pages="30--44",
abstract="Recommender systems can facilitate people to get effective information from the massive data, and it is the hot research currently in data mining. SVD++ is a kind of effective single model recommendation algorithm, which is based on the matrix decomposition combined with the neighborhood model. On the Spark, using the Stochastic Gradient Descent, this paper realized the distributed SVD++ algorithm through the Scala, deployed and applied the algorithm into an actual recommendation product for testing. The testing results represent that the distributed SVD++ algorithm succeeded in solving problems of terabytes of data processing in the e-commerce recommendation and the sparse data of user-item matrix, enhancing the quality in personalized commodity recommendation.",
isbn="978-981-10-0421-6"
}

@article{attention,
  author    = {Andrea Galassi and
               Marco Lippi and
               Paolo Torroni},
  title     = {Attention, please! {A} Critical Review of Neural Attention Models
               in Natural Language Processing},
  journal   = {CoRR},
  volume    = {abs/1902.02181},
  year      = {2019},
  url       = {http://arxiv.org/abs/1902.02181},
  archivePrefix = {arXiv},
  eprint    = {1902.02181},
  timestamp = {Wed, 25 Sep 2019 17:52:35 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1902-02181.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{neuralonrecommendations,  author={Adomavicius, G. and Tuzhilin, A.},  journal={IEEE Transactions on Knowledge and Data Engineering},   title={Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions},   year={2005},  volume={17},  number={6},  pages={734-749},  doi={10.1109/TKDE.2005.99}}

@misc{wang2015collaborative,
      title={Collaborative Deep Learning for Recommender Systems}, 
      author={Hao Wang and Naiyan Wang and Dit-Yan Yeung},
      year={2015},
      eprint={1409.2944},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{deepicf,
  author    = {Feng Xue and
               Xiangnan He and
               Xiang Wang and
               Jiandong Xu and
               Kai Liu and
               Richang Hong},
  title     = {Deep Item-based Collaborative Filtering for Top-N Recommendation},
  journal   = {CoRR},
  volume    = {abs/1811.04392},
  year      = {2018},
  url       = {http://arxiv.org/abs/1811.04392},
  archivePrefix = {arXiv},
  eprint    = {1811.04392},
  timestamp = {Mon, 31 Aug 2020 18:56:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1811-04392.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@online{matrixvanilla,
  author = {Simon Funk},
  title = {Netflix prize},
  year = 2006,
  url = {https://sifter.org/~simon/journal/20061211.html},
  urldate = {2021-07-19}
}
@article{knn,
author = {Herlocker, Jon and Konstan, Joseph and Borchers, Al and Riedl, John},
year = {2017},
month = {08},
pages = {227-234},
title = {An Algorithmic Framework for Performing Collaborative Filtering},
volume = {51},
journal = {ACM SIGIR Forum},
doi = {10.1145/3130348.3130372}
}

@ARTICLE{matrix,  author={Koren, Yehuda and Bell, Robert and Volinsky, Chris},  journal={Computer},   title={Matrix Factorization Techniques for Recommender Systems},   year={2009},  volume={42},  number={8},  pages={30-37},  doi={10.1109/MC.2009.263}}

@inproceedings{latentspace,
author = {Agarwal, Deepak and Chen, Bee-Chung},
year = {2009},
month = {01},
pages = {19-28},
title = {Regression-based latent factor models},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
doi = {10.1145/1557019.1557029}
}

@inbook{contentbased,
author = {Lops, Pasquale and de Gemmis, Marco and Semeraro, Giovanni},
year = {2011},
month = {01},
pages = {73-105},
title = {Content-based Recommender Systems: State of the Art and Trends},
journal = {Recommender Systems Handbook},
doi = {10.1007/978-0-387-85820-3_3}
}

@book{vectorspacemodel,
  added-at = {2016-09-07T14:36:11.000+0200},
  author = {Baeza-Yates, Ricardo and Ribeiro-Neto, Berthier and others},
  biburl = {https://www.bibsonomy.org/bibtex/26f98674acaad1f1337a3bf440913b517/becker},
  interhash = {1f2af136a16967eca564e61bfc0a3958},
  intrahash = {6f98674acaad1f1337a3bf440913b517},
  keywords = {diss imported-verified inthesis microtrails},
  publisher = {ACM press New York},
  timestamp = {2017-01-03T11:05:32.000+0100},
  title = {Modern information retrieval},
  volume = 463,
  year = 1999
}

@inbook{tfidf, place={Cambridge}, title={Data Mining}, DOI={10.1017/CBO9781139058452.002}, booktitle={Mining of Massive Datasets}, publisher={Cambridge University Press}, author={Rajaraman, Anand and Ullman, Jeffrey David}, year={2011}, pages={1–17}}

@phdthesis{contentprofile, series={TRITA-ICT-EX}, title={Content-based Recommender System for Movie Website}, url={http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-188494}, abstractNote={Recommender System is a tool helping users find content and overcome information overload. It predicts interests of users and makes recommendation according to the interest model of users.The original content-based recommender system is the continuation and development of collaborative filtering, which doesn’t need the user’s evaluation for items. Instead, the similarity is calculated based on the information of items that are chose by users, and then make the recommendation accordingly. With the improvement of machine learning, current content-based recommender system can build profile for users and products respectively. Building or updating the profile according to the analysis of items that are bought or visited by users. The system can compare the user and the profile of items and then recommend the most similar products. So this recommender method that compare user and product directly cannot be brought into collaborative filtering model. The foundation of content-based algorithm is acquisition and quantitative analysis of the content. As the research of acquisition and filtering of text information are mature, many current content-based recommender systems make recommendation according to the analysis of text information.This paper introduces content-based recommender system for the movie website of VionLabs. There are a lot of features extracted from the movie, they are diversity and unique, which is also the difference from other recommender systems. We use these features to construct movie model and calculate similarity. We introduce a new approach for setting weight of features, which improves the representative of movies.Finally we evaluate the approach to illustrate the improvement.}, author={Ma, Ke}, year={2016}, collection={TRITA-ICT-EX} }

@InProceedings{content-embeddings,
author="Musto, Cataldo
and Semeraro, Giovanni
and de Gemmis, Marco
and Lops, Pasquale",
editor="Ferro, Nicola
and Crestani, Fabio
and Moens, Marie-Francine
and Mothe, Josiane
and Silvestri, Fabrizio
and Di Nunzio, Giorgio Maria
and Hauff, Claudia
and Silvello, Gianmaria",
title="Learning Word Embeddings from Wikipedia for Content-Based Recommender Systems",
booktitle="Advances in Information Retrieval",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="729--734",
abstract="In this paper we present a preliminary investigation towards the adoption of Word Embedding techniques in a content-based recommendation scenario. Specifically, we compared the effectiveness of three widespread approaches as Latent Semantic Indexing, Random Indexing and Word2Vec in the task of learning a vector space representation of both items to be recommended as well as user profiles.",
isbn="978-3-319-30671-1"
}

% spotifyembeddings
@inproceedings{spotifyembeddings,
author = {Hansen, Casper and Hansen, Christian and Maystre, Lucas and Mehrotra, Rishabh and Brost, Brian and Tomasi, Federico and Lalmas, Mounia},
year = {2020},
month = {09},
pages = {53-62},
title = {Contextual and Sequential User Embeddings for Large-Scale Music Recommendation},
doi = {10.1145/3383313.3412248}
}

@inproceedings{rnn-youtube,
author = {Covington, Paul and Adams, Jay and Sargin, Emre},
title = {Deep Neural Networks for YouTube Recommendations},
year = {2016},
isbn = {9781450340359},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2959100.2959190},
doi = {10.1145/2959100.2959190},
abstract = {YouTube represents one of the largest scale and most sophisticated industrial recommendation
systems in existence. In this paper, we describe the system at a high level and focus
on the dramatic performance improvements brought by deep learning. The paper is split
according to the classic two-stage information retrieval dichotomy: first, we detail
a deep candidate generation model and then describe a separate deep ranking model.
We also provide practical lessons and insights derived from designing, iterating and
maintaining a massive recommendation system with enormous user-facing impact.},
booktitle = {Proceedings of the 10th ACM Conference on Recommender Systems},
pages = {191–198},
numpages = {8},
keywords = {scalability, recommender system, deep learning},
location = {Boston, Massachusetts, USA},
series = {RecSys '16}
}

@inproceedings{rnn-youtube-improved,
title	= {Latent Cross: Making Use of Context in Recurrent Recommender Systems},
author	= {Alex Beutel and Paul Covington and Sagar Jain and Can Xu and Jia Li and Vince Gatto and Ed H. Chi},
year	= {2018},
booktitle	= {WSDM 2018: The Eleventh ACM International Conference on Web Search and Data Mining}
}

@inproceedings{alibaba,
author = {Chen, Qiwei and Zhao, Huan and Li, Wei and Huang, Pipei and Ou, Wenwu},
title = {Behavior Sequence Transformer for E-Commerce Recommendation in Alibaba},
year = {2019},
isbn = {9781450367837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3326937.3341261},
doi = {10.1145/3326937.3341261},
abstract = {Deep learning based methods have been widely used in industrial recommendation systems
(RSs). Previous works adopt an Embedding&amp;MLP paradigm: raw features are embedded into
low-dimensional vectors, which are then fed on to MLP for final recommendations. However,
most of these works just concatenate different features, ignoring the sequential nature
of users' behaviors. In this paper, we propose to use the powerful Transformer model
to capture the sequential signals underlying users' behavior sequences for recommendation
in Alibaba. Experimental results demonstrate the superiority of the proposed model,
which is then deployed online at Taobao and obtain significant improvements in online
Click-Through-Rate (CTR) comparing to two baselines.},
booktitle = {Proceedings of the 1st International Workshop on Deep Learning Practice for High-Dimensional Sparse Data},
articleno = {12},
numpages = {4},
location = {Anchorage, Alaska},
series = {DLP-KDD '19}
}

@book{pattern-bishop,
author = {Bishop, Christopher M.},
title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
year = {2006},
isbn = {0387310738},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

@article{neurons,
  author    = {J{\"{u}}rgen Schmidhuber},
  title     = {Deep Learning in Neural Networks: An Overview},
  journal   = {CoRR},
  volume    = {abs/1404.7828},
  year      = {2014},
  url       = {http://arxiv.org/abs/1404.7828},
  archivePrefix = {arXiv},
  eprint    = {1404.7828},
  timestamp = {Mon, 13 Aug 2018 16:47:28 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Schmidhuber14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{neuron-diagram,
author = {Yacim, Joseph and Boshoff, Douw},
year = {2018},
month = {11},
pages = {375-418},
title = {Impact of Artificial Neural Networks Training Algorithms on Accurate Prediction of Property Values},
volume = {40},
journal = {Journal of Real Estate Research},
doi = {10.1080/10835547.2018.12091505}
}

@inproceedings{relu,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units.
These can be generalized by replacing each binary unit by an infinite number of copies
that all have the same weights but have progressively more negative biases. The learning
and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated
efficiently by noisy, rectified linear units. Compared with binary units, these units
learn features that are better for object recognition on the NORB dataset and face
verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified
linear units preserve information about relative intensities as information travels
through multiple layers of feature detectors.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@inproceedings{lrelu,
  title={Rectifier Nonlinearities Improve Neural Network Acoustic Models},
  author={Andrew L. Maas},
  year={2013}
}

@article{backprop,
  title={Learning representations by back-propagating errors},
  author={D. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536}
}
@inproceedings{rnn-unfolded,
author = {Feng, Weijiang and Guan, Naiyang and Li, Yuan and Zhang, Xiang and Luo, Zhigang},
year = {2017},
month = {05},
pages = {681-688},
title = {Audio visual speech recognition with multimodal recurrent neural networks},
doi = {10.1109/IJCNN.2017.7965918}
}

@MISC{rnn-hand,
    author = {Alex Graves and Marcus Liwicki and Santiago Fernandez and Roman Bertolami and Horst Bunke and Jürgen Schmidhuber},
    title = {A Novel Connectionist System for Unconstrained Handwriting Recognition},
    year = {2008}
}

@article{rnn-speech,
  author    = {Hasim Sak and
               Andrew W. Senior and
               Fran{\c{c}}oise Beaufays},
  title     = {Long Short-Term Memory Based Recurrent Neural Network Architectures
               for Large Vocabulary Speech Recognition},
  journal   = {CoRR},
  volume    = {abs/1402.1128},
  year      = {2014},
  url       = {http://arxiv.org/abs/1402.1128},
  archivePrefix = {arXiv},
  eprint    = {1402.1128},
  timestamp = {Mon, 13 Aug 2018 16:47:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/SakSB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lstm-author,
    author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
    title = "{Long Short-Term Memory}",
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = "{Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}





@article{LSTM_image,
  author    = {Jeff Donahue and
               Lisa Anne Hendricks and
               Sergio Guadarrama and
               Marcus Rohrbach and
               Subhashini Venugopalan and
               Kate Saenko and
               Trevor Darrell},
  title     = {Long-term Recurrent Convolutional Networks for Visual Recognition
               and Description},
  journal   = {CoRR},
  volume    = {abs/1411.4389},
  year      = {2014},
  url       = {http://arxiv.org/abs/1411.4389},
  archivePrefix = {arXiv},
  eprint    = {1411.4389},
  timestamp = {Wed, 07 Jun 2017 14:41:44 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/DonahueHGRVSD14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{gru,
  author    = {Kyunghyun Cho and
               Bart van Merrienboer and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Fethi Bougares and
               Holger Schwenk and
               Yoshua Bengio},
  title     = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical
               Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1406.1078},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.1078},
  archivePrefix = {arXiv},
  eprint    = {1406.1078},
  timestamp = {Mon, 13 Aug 2018 16:46:44 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ChoMGBSB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{daniel, title={Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM) & Gated Recurrent Unit (GRU)}, url={http://dprogrammer.org/rnn-lstm-gru}, journal={dprogrammer}, author={Daniel Lopez}, year={2015}, month={Sep}}


@INPROCEEDINGS{gru-forget,  author={Gers, F.A. and Schmidhuber, J. and Cummins, F.},  booktitle={1999 Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470)},   title={Learning to forget: continual prediction with LSTM},   year={1999},  volume={2},  number={},  pages={850-855 vol.2},  doi={10.1049/cp:19991218}}

@article{gru-speech,
   title={Light Gated Recurrent Units for Speech Recognition},
   volume={2},
   ISSN={2471-285X},
   url={http://dx.doi.org/10.1109/TETCI.2017.2762739},
   DOI={10.1109/tetci.2017.2762739},
   number={2},
   journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Ravanelli, Mirco and Brakel, Philemon and Omologo, Maurizio and Bengio, Yoshua},
   year={2018},
   month={Apr},
   pages={92–102}
}

@article{encoder-decoder,
  author    = {Ramesh Nallapati and
               Bing Xiang and
               Bowen Zhou},
  title     = {Sequence-to-Sequence RNNs for Text Summarization},
  journal   = {CoRR},
  volume    = {abs/1602.06023},
  year      = {2016},
  url       = {http://arxiv.org/abs/1602.06023},
  archivePrefix = {arXiv},
  eprint    = {1602.06023},
  timestamp = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NallapatiXZ16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{attention,
  title={Neural Machine Translation by Jointly Learning to Align and Translate},
  author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
  journal={CoRR},
  year={2015},
  volume={abs/1409.0473}
}

@mastersthesis{attentionplots,
   author = {Jonsson, Fredrik},
   institution = {KTH, School of Electrical Engineering and Computer Science (EECS)},
   pages = {38},
   school = {KTH, School of Electrical Engineering and Computer Science (EECS)},
   title = {Evaluation of the Transformer Model for Abstractive Text Summarization},
   series = {TRITA-EECS-EX},
   number = {2019:563},
   abstract = {Being able to generate summaries automatically could speed up the spread and retention of information and potentially increase productivity in several fields. Using RNN-based encoder-decoder models with attention have been successful on a variety of language-related tasks such as automatic summarization but also in the field of machine translation. Lately, the Transformer model has been shown to outperform RNN-based models with attention in the relatedfield of machine translation. This study compares the Transformer model to a LSTM-based encoderdecoder model with attention on the task of abstractive summarization. Evaluation is done both automatically, using ROUGE score, as well as using human evaluators to estimate the grammar and readability of the generated summaries. The results show that the Transformer model produces better summaries both in terms of ROUGE score and when evaluated with human evaluators. },
   year = {2019}
}

@article{attentionisallyouneed,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{embeddingsPre1,
title = {Parallel distributed processing: explorations in the microstructure of cognition. Volume 1. Foundations},
author = {Rumelhart, D E and Mcclelland, J L},
abstractNote = {The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.},
doi = {},
url = {https://www.osti.gov/biblio/5838709},
place = {United States},
year = {1986},
month = {1}
}
@article{embeddingsPre2,
  title={Learning representations by back-propagating errors},
  author={D. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  journal={Nature},
  year={1986},
  volume={323},
  pages={533-536}
}


@article{embeddingsPre3,
title = {Finding structure in time},
journal = {Cognitive Science},
volume = {14},
number = {2},
pages = {179-211},
year = {1990},
issn = {0364-0213},
doi = {https://doi.org/10.1016/0364-0213(90)90002-E},
url = {https://www.sciencedirect.com/science/article/pii/036402139090002E},
author = {Jeffrey L. Elman},
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.}
}


@inproceedings{word2vec,
author = {Mikolov, Tomas and Corrado, G.s and Chen, Kai and Dean, Jeffrey},
year = {2013},
month = {01},
pages = {1-12},
title = {Efficient Estimation of Word Representations in Vector Space}
}

@inproceedings{noveltyintro,
author = {Vargas, Sa\'{u}l},
title = {Novelty and Diversity Enhancement and Evaluation in Recommender Systems and Information Retrieval},
year = {2014},
isbn = {9781450322577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2600428.2610382},
doi = {10.1145/2600428.2610382},
abstract = {The development and evaluation of Information Retrieval and Recommender Systems has
traditionally focused on the relevance and accuracy of retrieved documents and recommendations,
respectively. However, there is an increasing realization that accuracy alone might
be a sub-optimal strategy for a successful user experience. Properties such as novelty
and diversity have been explored in both fields for assessing and enhancing the usefulness
of search results and recommendations. In this doctoral research we study the assessment
and enhancement of both properties in the confluence of Information Retrieval and
Recommender Systems.},
booktitle = {Proceedings of the 37th International ACM SIGIR Conference on Research &amp; Development in Information Retrieval},
pages = {1281},
numpages = {1},
keywords = {recommender systems, diversity, novelty},
location = {Gold Coast, Queensland, Australia},
series = {SIGIR '14}
}

@inproceedings{noveltyintro,
author = {Konstan, Joseph and McNee, Sean and Ziegler, Cai-Nicolas and Torres, Roberto and Kapoor, Nishikant and Riedl, John},
year = {2006},
month = {01},
pages = {},
title = {Lessons on Applying Automated Recommender Systems to Information-Seeking Tasks}
}

@article{noveltyassumption,
author = {Jones, Nicolas and Pu, Pearl},
year = {2007},
month = {01},
pages = {},
title = {User Technology Adoption Issues in Recommender Systems}
}

@misc{sun2019bert4rec,
      title={BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer}, 
      author={Fei Sun and Jun Liu and Jian Wu and Changhua Pei and Xiao Lin and Wenwu Ou and Peng Jiang},
      year={2019},
      eprint={1904.06690},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{ruder2019transfer,
  title={Transfer learning in natural language processing},
  author={Ruder, Sebastian and Peters, Matthew E and Swayamdipta, Swabha and Wolf, Thomas},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Tutorials},
  pages={15--18},
  year={2019}
}

@mastersthesis{hm2vec,
   author = {Deligiorgis, Georgios},
   institution = {KTH, School of Electrical Engineering and Computer Science (EECS)},
   pages = {78},
   school = {KTH, School of Electrical Engineering and Computer Science (EECS)},
   title = {Context-Aware Graph Convolutional Network with Multi-Clusters Mini-Batch for Link Prediction},
   series = {TRITA-EECS-EX},
   number = {2020:671},
   abstract = {Predicting which fashion items can compose an outfit is not a trivial task since every person has different preferences based on their experiences, location, etc., determining each personal style. The items that can formulate an outfit have to be compatible, but the compatibility of two products should vary based on the context of the products, where context is defined as the products that we already know that are compatible with them. The authors in [1] introduced a model named Context-Aware-Graph-Convolutional-Network (CA-GCN), where they predict the compatibility among fashion items, by taking into consideration their context, after being trained with full-batch Gradient Descent (GD), making its application more difficult on denser and larger graphs. The authors in [2] have proposed the Cluster-GCN method to train GCN [3] models with mini-batch GD for node classification. We propose a novel model named CA-Cluster-GCN that merges a modified version of the Cluster-GCN with the CA-GCN model. We modify the Cluster-GCN in such a way that we can apply it for link-prediction since the CA-GCN predicts the compatibility among the fashion items by predicting if an edge exists among them. Our CA-Cluster-GCN achieves State-Of-The-Art performance on the public Maryland Polyvore [4] dataset and it outperforms the CA-GCN model on the public Amazon [5, 6] dataset. Additional tests have been performed on the industrial datasets provided by H&amp;M. },
   year = {2020}
}

@inproceedings{glove,
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
year = {2014},
month = {01},
pages = {1532-1543},
title = {Glove: Global Vectors for Word Representation},
volume = {14},
journal = {EMNLP},
doi = {10.3115/v1/D14-1162}
}

@article{xDeepFM,
   title={xDeepFM},
   ISBN={9781450355520},
   url={http://dx.doi.org/10.1145/3219819.3220023},
   DOI={10.1145/3219819.3220023},
   journal={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining},
   publisher={ACM},
   author={Lian, Jianxun and Zhou, Xiaohuan and Zhang, Fuzheng and Chen, Zhongxia and Xie, Xing and Sun, Guangzhong},
   year={2018},
   month={Jul}
}

